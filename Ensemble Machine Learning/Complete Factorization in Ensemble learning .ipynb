{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16c7c39e",
   "metadata": {},
   "source": [
    "# Fact in Ensemble Learning : \n",
    "    \n",
    "    Ensemble learning is a machine learning technique that combines multiple models, known as base learners or weak learners, to make predictions or decisions. One of the common techniques used in ensemble learning is called \"factoring\" or \"factorization.\"\n",
    "\n",
    "Factoring in ensemble learning refers to the process of decomposing the overall prediction task into subtasks or factors, and then training individual base learners to handle each factor separately. This decomposition allows each base learner to focus on a specific aspect or subset of the data, potentially improving the overall predictive performance of the ensemble.\n",
    "\n",
    "Here is a step-by-step explanation of the factoring process in ensemble learning:\n",
    "\n",
    "(1).  Decomposition: The first step is to decompose the prediction task into its constituent factors. These factors can be different aspects, subsets, or representations of the data that are relevant to the prediction problem. For example, in a computer vision task, factors can be decomposed based on color, texture, shape, or any other distinguishing characteristics of the images.\n",
    "    \n",
    "\n",
    "(2).  Base Learner Training: Once the factors are identified, individual base learners are trained to handle each factor separately. Each base learner is responsible for learning the patterns and relationships within its assigned factor. The choice of base learners can vary depending on the ensemble learning algorithm being used, but commonly used base learners include decision trees, neural networks, support vector machines, or k-nearest neighbors.\n",
    "    \n",
    "\n",
    "(3).  Factor Combination: After training the base learners, the next step is to combine their predictions to obtain the final ensemble prediction. This can be done through various methods, such as majority voting, weighted voting, averaging, or stacking. The combination technique should take into account the strengths and weaknesses of each base learner and aim to leverage their collective knowledge for improved predictions.\n",
    "    \n",
    "\n",
    "(4).  Ensemble Prediction: The combined predictions from the base learners form the ensemble prediction. Depending on the nature of the prediction problem, the ensemble prediction can be a class label, a probability distribution, or a regression value.\n",
    "    \n",
    "\n",
    "(5).  Evaluation and Iteration: The final step involves evaluating the performance of the ensemble on a validation or test set. Metrics such as accuracy, precision, recall, or mean squared error can be used to assess the ensemble's predictive capabilities. If the performance is satisfactory, the ensemble can be deployed for making predictions on unseen data. However, if the performance is not up to the desired level, the factoring process can be refined by adjusting the factors or incorporating different base learners.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248d2bad",
   "metadata": {},
   "source": [
    "# Advantages of Fectorization :\n",
    "    \n",
    "\n",
    "(1).  Specialization: Factoring allows individual base learners to specialize in specific factors or subtasks of the prediction problem. By decomposing the task, each base learner can focus on learning the patterns and relationships within its assigned factor, potentially improving the accuracy and performance of the ensemble.\n",
    "\n",
    "(2).  Complementary Knowledge: Different base learners may capture different aspects of the data and have diverse strengths and weaknesses. By factoring, the ensemble can combine the diverse knowledge and expertise of the base learners, leading to a more comprehensive understanding of the data and improved prediction accuracy.\n",
    "\n",
    "(3).  Complexity Management: Factoring can help manage complex prediction tasks by breaking them down into simpler subtasks. By decomposing the problem, the ensemble learning process becomes more manageable and easier to handle, as individual base learners can focus on specific factors without being overwhelmed by the entire problem.\n",
    "\n",
    "(4).  Error Reduction: If one or more base learners perform poorly on certain factors, factoring allows for the identification of weak areas. This enables improvements by either refining the factors or selecting more appropriate base learners, potentially reducing prediction errors and improving the overall performance of the ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c194ed80",
   "metadata": {},
   "source": [
    "# Disadvantages of Factorization : \n",
    "    \n",
    "(1). Increased Complexity: Factoring introduces additional complexity to the ensemble learning process. Decomposing the problem into factors requires careful consideration and domain expertise. It may also require additional computational resources and time to train and combine the base learners.\n",
    "\n",
    "\n",
    "(2). Factor Selection: Choosing the right factors for decomposition is crucial for the success of the ensemble. Selecting inappropriate factors or missing important factors can lead to suboptimal results. It requires domain knowledge and understanding of the data to identify the factors that truly contribute to the prediction task.\n",
    "\n",
    "\n",
    "(3). Overfitting Risks: When factoring, there is a risk of overfitting if the base learners become too specialized and learn noise or irrelevant patterns within their assigned factors. Regularization techniques and proper validation strategies should be employed to mitigate this risk and ensure generalization performance.\n",
    "\n",
    "\n",
    "(4). Interpretability: Ensemble models can become less interpretable due to the complexity introduced by factorization. While individual base learners may be easier to interpret, the combination of their predictions may lack transparency, making it challenging to understand the reasoning behind the ensemble's decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4d1a2f",
   "metadata": {},
   "source": [
    "# Example - 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cafafc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Accuracy: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the factors (subsets of features)\n",
    "factor_1 = [0, 1]  # Sepal length and sepal width\n",
    "factor_2 = [2, 3]  # Petal length and petal width\n",
    "\n",
    "# Train base learners on their respective factors\n",
    "base_learner_1 = DecisionTreeClassifier()\n",
    "base_learner_1.fit(X_train[:, factor_1], y_train)\n",
    "\n",
    "base_learner_2 = DecisionTreeClassifier()\n",
    "base_learner_2.fit(X_train[:, factor_2], y_train)\n",
    "\n",
    "# Make predictions with base learners\n",
    "predictions_1 = base_learner_1.predict(X_test[:, factor_1])\n",
    "predictions_2 = base_learner_2.predict(X_test[:, factor_2])\n",
    "\n",
    "# Combine predictions using majority voting\n",
    "ensemble_predictions = np.array([predictions_1, predictions_2])\n",
    "final_predictions = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=ensemble_predictions)\n",
    "\n",
    "# Evaluate the ensemble's accuracy\n",
    "ensemble_accuracy = accuracy_score(y_test, final_predictions)\n",
    "print(\"Ensemble Accuracy:\", ensemble_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1334e71",
   "metadata": {},
   "source": [
    "# Example - 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20336d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Accuracy: 0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Breast Cancer Wisconsin dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the factors (subsets of features)\n",
    "factor_1 = [0, 1, 2, 3]  # First four features\n",
    "factor_2 = [4, 5, 6, 7]  # Next four features\n",
    "factor_3 = [8, 9, 10, 11]  # Next four features\n",
    "factor_4 = [12, 13, 14, 15]  # Last four features\n",
    "\n",
    "# Train base learners on their respective factors\n",
    "base_learner_1 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "base_learner_1.fit(X_train[:, factor_1], y_train)\n",
    "\n",
    "base_learner_2 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "base_learner_2.fit(X_train[:, factor_2], y_train)\n",
    "\n",
    "base_learner_3 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "base_learner_3.fit(X_train[:, factor_3], y_train)\n",
    "\n",
    "base_learner_4 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "base_learner_4.fit(X_train[:, factor_4], y_train)\n",
    "\n",
    "# Make predictions with base learners\n",
    "predictions_1 = base_learner_1.predict(X_test[:, factor_1])\n",
    "predictions_2 = base_learner_2.predict(X_test[:, factor_2])\n",
    "predictions_3 = base_learner_3.predict(X_test[:, factor_3])\n",
    "predictions_4 = base_learner_4.predict(X_test[:, factor_4])\n",
    "\n",
    "# Combine predictions using voting\n",
    "ensemble_predictions = []\n",
    "for pred_1, pred_2, pred_3, pred_4 in zip(predictions_1, predictions_2, predictions_3, predictions_4):\n",
    "    votes = [pred_1, pred_2, pred_3, pred_4]\n",
    "    majority_vote = max(set(votes), key=votes.count)\n",
    "    ensemble_predictions.append(majority_vote)\n",
    "\n",
    "# Evaluate the ensemble's accuracy\n",
    "ensemble_accuracy = accuracy_score(y_test, ensemble_predictions)\n",
    "print(\"Ensemble Accuracy:\", ensemble_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a8f1c9",
   "metadata": {},
   "source": [
    "# In which scenerio I have to use factorization ?\n",
    "\n",
    "(1). High-Dimensional Data: When dealing with high-dimensional data, factorization can be useful to break down the problem into more manageable subsets of features. By factoring the task, each base learner can focus on a subset of features, reducing the complexity and potential overfitting.\n",
    "\n",
    "\n",
    "(2). Heterogeneous Data: In cases where the dataset contains different types of features or diverse sources of data, factorization can help handle the heterogeneity. By factoring based on the different types or sources of data, base learners can specialize in capturing patterns and relationships within their assigned factors, leading to improved ensemble performance.\n",
    "\n",
    "\n",
    "(3). Complex Prediction Tasks: Factorization can be valuable for tackling complex prediction tasks by decomposing them into simpler subtasks. Each base learner can focus on learning specific factors or subproblems, leading to improved accuracy and performance. This is especially useful when the complete problem is difficult to model or lacks a clear global solution.\n",
    "\n",
    "    \n",
    "(4). Resource Constraints: In situations where computational resources or time are limited, factorization can help manage the complexity of the ensemble learning process. By dividing the task into factors, individual base learners can be trained and combined separately, reducing the overall computational burden and enabling scalability.\n",
    "\n",
    "\n",
    "(5). Feature Importance Analysis: Factoring can be useful for analyzing the importance of different subsets of features. By training base learners on different factors, you can assess the relative contributions of each factor to the overall prediction task. This analysis can provide insights into feature relevance and guide feature selection or engineering efforts.\n",
    "\n",
    "\n",
    "(6). Addressing Model Bias: Factorization can be employed to address model bias or limitations. By factoring the task, you can identify weak areas or biases within specific factors and make targeted improvements. This allows for a more refined and balanced ensemble, potentially reducing bias and improving overall prediction performance.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556cd282",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
