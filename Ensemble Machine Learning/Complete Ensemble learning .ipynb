{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31d0c4d8",
   "metadata": {},
   "source": [
    "# Ensemble learning is a machine learning technique that involves combining multiple individual models, known as base learners, to make predictions or decisions. The main idea behind ensemble learning is that by combining the predictions of multiple models, the overall performance can be improved compared to using a single model.\n",
    "\n",
    "\n",
    "Ensemble learning can be used for both classification and regression problems, and it is particularly effective when the base learners are diverse and make different types of errors. There are several popular ensemble methods, including bagging, boosting, and stacking. Let's explore each of them in more detail:\n",
    "\n",
    "\n",
    "(1).  Bagging: Bagging stands for Bootstrap Aggregating. It involves creating multiple subsets of the training data through bootstrapping (sampling with replacement), training a separate base learner on each subset, and then combining their predictions. The most common example of bagging is the Random Forest algorithm, which combines multiple decision trees.\n",
    "    \n",
    "\n",
    "(2).  Boosting: Boosting is an iterative ensemble method that focuses on training weak learners sequentially and giving more importance to the instances that were misclassified by previous learners. In boosting, each base learner is trained to correct the mistakes made by the previous learners. Examples of boosting algorithms include AdaBoost, Gradient Boosting, and XGBoost.\n",
    "    \n",
    "\n",
    "(3).  Stacking: Stacking (short for stacked generalization) involves training multiple base learners and then combining their predictions using another model called a meta-learner or blender. The base learners' predictions are used as input features for the meta-learner, which learns to make the final prediction. Stacking can be seen as a two-level learning process, where the base learners form the first level, and the meta-learner forms the second level.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fbb821",
   "metadata": {},
   "source": [
    "# Advantages of Ensemble Learning \n",
    "\n",
    "(1). Improved performance: Ensemble methods can often achieve better performance than individual models, especially when the base learners are diverse and complementary.\n",
    "    \n",
    "\n",
    "(2). Robustness: Ensemble learning can be more robust to noisy data and outliers because errors made by individual models can be compensated by others.\n",
    "    \n",
    "\n",
    "(3). Reducing overfitting: Ensemble methods can help reduce overfitting, as the combination of multiple models reduces the risk of relying too heavily on the idiosyncrasies of a single model.\n",
    "    \n",
    "\n",
    "(4). Versatility: Ensemble learning can be applied to a wide range of machine learning algorithms and models, including decision trees, neural networks, and support vector machines.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7e7670",
   "metadata": {},
   "source": [
    "# Disadvantages of Ensemble Learning \n",
    "\n",
    "(1).  Increased complexity: Ensemble methods can be more computationally expensive and require more memory compared to training a single model.\n",
    " \n",
    "(2).  Interpretability: The predictions of ensemble models can be harder to interpret than those of individual models, as they involve combining multiple predictions.\n",
    "\n",
    "\n",
    "(3). Training time: Training multiple base learners can be time-consuming, especially for large datasets or complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a71c2c",
   "metadata": {},
   "source": [
    "# (1). Bagging "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78480289",
   "metadata": {},
   "source": [
    "# Bagging (Bootstrap Aggregating) is an ensemble learning method that combines multiple base learners by training them on different subsets of the training data using bootstrapping. Each base learner is trained independently, and their predictions are combined to make the final prediction.\n",
    "\n",
    "\n",
    "(1). Bagging Process:\n",
    "\n",
    "(a). Data Sampling: Bagging starts by creating multiple random subsets (samples) of the training data using bootstrapping. Bootstrapping involves sampling the training data with replacement, which means that each subset can contain duplicate instances and may not include all instances.\n",
    "    \n",
    "(b). Base Learner Training: Each subset is used to train a separate base learner independently. The base learners can be any model capable of making predictions, such as decision trees, support vector machines, or neural networks. Each base learner is trained on its respective subset of the data.\n",
    "    \n",
    "(c). Prediction Aggregation: Once the base learners are trained, their predictions are combined to make the final prediction. For classification problems, the most common aggregation method is voting, where the majority vote among the base learners' predictions determines the final prediction. For regression problems, the predictions are typically averaged.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0535e8e",
   "metadata": {},
   "source": [
    "# Advantages of Bagging \n",
    "\n",
    "(1). Improved Generalization: Bagging reduces overfitting by creating diverse subsets of the data through bootstrapping. Each base learner focuses on a different subset, resulting in diverse models that collectively make more accurate predictions.\n",
    "\n",
    "\n",
    "(2). Increased Stability: Bagging reduces the impact of individual outliers or noisy instances as they are less likely to appear in all subsets. By averaging or voting over multiple predictions, bagging improves the stability and robustness of the model.\n",
    "\n",
    "\n",
    "\n",
    "(3). Parallelization: Since each base learner is trained independently, bagging is well-suited for parallel computing. This allows for efficient training on multi-core processors or distributed computing environments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a83ac3",
   "metadata": {},
   "source": [
    "# Disadvantages of Bagging:\n",
    "\n",
    "(1). Interpretability: Bagging can make the final prediction harder to interpret because it involves combining the predictions of multiple base learners. The focus is on improving accuracy rather than providing explainable insights.\n",
    "    \n",
    "(2). Increased Complexity: Bagging increases the complexity of the model by training multiple base learners and aggregating their predictions. This can result in higher computational requirements and memory usage compared to a single model.\n",
    "    \n",
    "(3). Training Time: Training multiple base learners can be time-consuming, especially for large datasets or complex models. However, the training time can be reduced through parallelization.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29077a2",
   "metadata": {},
   "source": [
    "# Popular Bagging Algorithm: Random Forest is a well-known bagging algorithm that uses decision trees as base learners. It combines the predictions of multiple decision trees to make the final prediction. Random Forest addresses the limitations of individual decision trees, such as overfitting and sensitivity to data variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72753240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize a list to store the base learners\n",
    "base_learners = []\n",
    "\n",
    "# Number of base learners (decision trees)\n",
    "num_base_learners = 10\n",
    "\n",
    "# Train the base learners\n",
    "for i in range(num_base_learners):\n",
    "    # Create a bootstrap sample of the training data\n",
    "    bootstrap_indices = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "    X_bootstrap = X_train[bootstrap_indices]\n",
    "    y_bootstrap = y_train[bootstrap_indices]\n",
    "    \n",
    "    # Create and train a base learner (Random Forest)\n",
    "    base_learner = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "    base_learner.fit(X_bootstrap, y_bootstrap)\n",
    "    \n",
    "    # Add the trained base learner to the list\n",
    "    base_learners.append(base_learner)\n",
    "\n",
    "# Make predictions with each base learner\n",
    "base_predictions = []\n",
    "for base_learner in base_learners:\n",
    "    y_pred = base_learner.predict(X_test)\n",
    "    base_predictions.append(y_pred)\n",
    "\n",
    "# Combine the predictions using majority voting\n",
    "ensemble_predictions = np.round(np.mean(base_predictions, axis=0))\n",
    "\n",
    "# Calculate the accuracy of the ensemble predictions\n",
    "accuracy = accuracy_score(y_test, ensemble_predictions)\n",
    "print(\"Ensemble Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c20336",
   "metadata": {},
   "source": [
    "# What is Base Learners ? \n",
    "\n",
    "In ensemble learning, the base learners, also known as weak learners, are the individual models that are combined to form the ensemble. Each base learner is trained independently on subsets of the data or with specific characteristics to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41e50847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Accuracy: 0.865\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize a list to store the base learners\n",
    "base_learners = []\n",
    "\n",
    "# Number of base learners (decision trees)\n",
    "num_base_learners = 10\n",
    "\n",
    "# Train the base learners\n",
    "for i in range(num_base_learners):\n",
    "    # Create and train a base learner (Decision Tree)\n",
    "    base_learner = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "    base_learner.fit(X_train, y_train)\n",
    "    \n",
    "    # Add the trained base learner to the list\n",
    "    base_learners.append(base_learner)\n",
    "\n",
    "# Make predictions with each base learner\n",
    "base_predictions = []\n",
    "for base_learner in base_learners:\n",
    "    y_pred = base_learner.predict(X_test)\n",
    "    base_predictions.append(y_pred)\n",
    "\n",
    "# Combine the predictions using majority voting\n",
    "ensemble_predictions = np.round(np.mean(base_predictions, axis=0))\n",
    "\n",
    "# Calculate the accuracy of the ensemble predictions\n",
    "accuracy = accuracy_score(y_test, ensemble_predictions)\n",
    "print(\"Ensemble Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c524cc9c",
   "metadata": {},
   "source": [
    "# How I set max_depth in DeicsionTreeClassifier? \n",
    "\n",
    "Setting the max_depth parameter in the DecisionTreeClassifier allows you to control the maximum depth or the maximum number of levels in the decision tree. It influences the complexity and generalization of the tree. Here's a detailed explanation of how you can set the max_depth parameter:\n",
    "\n",
    "\n",
    "(1).   Understanding the concept of max_depth:\n",
    "\n",
    "(a). The max_depth parameter specifies the maximum depth allowed for the decision tree during the training process.\n",
    "\n",
    "(b). Depth refers to the length of the longest path from the root node to any leaf node in the decision tree.\n",
    "\n",
    "(c). A shallow tree with a smaller max_depth value limits the number of splits and simplifies the decision-making process.\n",
    "\n",
    "(d). A deeper tree with a larger max_depth value can capture more complex patterns in the data but may also lead to overfitting.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "(2).   Approach to setting max_depth:\n",
    "\n",
    "(a). To determine the appropriate value for max_depth, consider the complexity of your dataset, the number of features, and the number of instances.\n",
    "\n",
    "(b). A small max_depth value can help prevent overfitting and improve generalization if your dataset is small or contains noisy or limited information.\n",
    "\n",
    "(c). For large datasets or when you have more features and instances, a higher max_depth value might be considered to capture more intricate patterns.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "(3).  Strategies to determine the optimal max_depth:\n",
    "\n",
    "(a).  Cross-Validation: Use techniques like k-fold cross-validation to evaluate the performance of the decision tree with different max_depth values. Select the value that provides the best trade-off between bias and variance.\n",
    "    \n",
    "(b).  Grid Search: Perform a grid search to systematically evaluate the decision tree model with various max_depth values. This method exhaustively searches for the optimal hyperparameters based on a specified evaluation metric (e.g., accuracy, F1 score).\n",
    "    \n",
    "(c).  Learning Curves: Plot learning curves for different max_depth values to visualize the relationship between training and validation performance. Identify the point where increasing max_depth does not significantly improve the model's performance on unseen data.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c28aa940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a decision tree classifier\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "\n",
    "# Define the parameter grid for max_depth\n",
    "param_grid = {'max_depth': [3, 5, 7, 10]}\n",
    "\n",
    "# Perform grid search to find the best max_depth value\n",
    "grid_search = GridSearchCV(decision_tree, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best max_depth value\n",
    "best_max_depth = grid_search.best_params_['max_depth']\n",
    "best_max_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b274d1",
   "metadata": {},
   "source": [
    "# Real life Use Case of Bagging \n",
    "\n",
    "Bagging, short for Bootstrap Aggregating, is a popular ensemble learning technique that combines the predictions of multiple models trained on different subsets of the data. Bagging can be used in various real-life scenarios across different domains. Here are a few examples:\n",
    "\n",
    "(1).   Random Forests: Random Forests is a well-known ensemble method that utilizes bagging. It combines multiple decision trees trained on different random subsets of the training data. Random Forests are widely used in applications such as:\n",
    "\n",
    "(a).   Predictive modeling: Random Forests are effective for tasks like classification and regression. They can be used for predicting customer churn, fraud detection, or forecasting stock prices.\n",
    "    \n",
    "(b).   Image and object recognition: Random Forests can be employed for tasks like image classification, object detection, and facial recognition.\n",
    "    \n",
    "(c).   Bioinformatics: Random Forests are applied to analyze genetic data, identify disease markers, and classify gene expression patterns.\n",
    "    \n",
    "(2).   Anomaly Detection: Bagging can be used for anomaly detection where the objective is to identify unusual or abnormal instances in a dataset. By training multiple models on different subsets of the data, bagging can help capture diverse patterns and distinguish anomalies from normal behavior. Applications include:\n",
    "\n",
    "(a).  Network intrusion detection: Bagging can be used to build an ensemble of models that detect unusual network traffic patterns, identifying potential security threats or attacks.\n",
    "    \n",
    "(b).  Fraud detection: Bagging-based models can be employed to detect fraudulent transactions, abnormal user behavior, or credit card fraud.\n",
    "    \n",
    "(c).  Health monitoring: Bagging techniques can be applied to monitor medical data, identifying abnormal health conditions or anomalies in patient monitoring.\n",
    "    \n",
    "(3).  Ensemble of Classifiers: Bagging can also be used to combine multiple classifiers to improve overall classification performance. Each base classifier is trained on a different subset of the data, and their predictions are aggregated using majority voting or averaging. This approach finds application in various areas, such as:\n",
    "        \n",
    "\n",
    "(a).  Text categorization: Bagging can be utilized to create an ensemble of text classifiers for tasks like sentiment analysis, spam detection, or topic classification.\n",
    "    \n",
    "(b).  Customer segmentation: Bagging-based ensemble classifiers can be employed to segment customers into different groups based on their behavior, preferences, or demographics.\n",
    "    \n",
    "(c).  Medical diagnosis: Bagging techniques can be used to build an ensemble of classifiers for medical diagnosis, combining the expertise of multiple models to improve accuracy.\n",
    "    \n",
    "    \n",
    "In summary, bagging finds practical applications in various domains, including predictive modeling, anomaly detection, and ensemble classification. Its ability to reduce variance, handle complex relationships, and improve overall model performance makes it a valuable technique in real-life scenarios where accurate predictions and reliable anomaly detection are desired."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafa744a",
   "metadata": {},
   "source": [
    "# Example -2 (MNIST DataSet(Hand Written Digits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9c87ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Accuracy: 0.9450714285714286\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Fetch the MNIST dataset\n",
    "mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(mnist.data, mnist.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize a base classifier (Decision Tree)\n",
    "base_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Initialize a bagging classifier\n",
    "bagging_classifier = BaggingClassifier(base_classifier, n_estimators=10, random_state=42)\n",
    "\n",
    "# Train the bagging classifier\n",
    "bagging_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = bagging_classifier.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the bagging classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Bagging Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2dd40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset (example: IMDb movie reviews)\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\saurabh\\\\Desktop\\\\pyth\\\\IMDB Dataset.csv\")\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize a base classifier (Decision Tree)\n",
    "base_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Initialize a bagging classifier\n",
    "bagging_classifier = BaggingClassifier(base_classifier, n_estimators=10, random_state=42)\n",
    "\n",
    "# Vectorize the text data using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Train the bagging classifier\n",
    "bagging_classifier.fit(X_train_vec, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = bagging_classifier.predict(X_test_vec)\n",
    "\n",
    "# Calculate the accuracy of the bagging classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Bagging Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaef3c45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
